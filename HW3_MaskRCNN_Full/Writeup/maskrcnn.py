# -*- coding: utf-8 -*-
"""BeastMode_MaskRCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zoAkk6p9jFfMfbkNgnYiuhD8SrVwcFuP

## Imports
"""

import torch, torchvision, cv2, itertools, collections, os, pdb, h5py

import skimage
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.utils.rnn as rnn
import torch.optim as optim
import matplotlib.pyplot as plt

from datetime import datetime
from google.colab import drive, files
from torchsummary import summary
from torch.utils.data import DataLoader
from torch.utils.data.sampler import SubsetRandomSampler
from torchvision.utils import make_grid
from torchvision import transforms
from torchvision.models.detection.image_list import ImageList 
from torchvision.ops import nms

from timeit import default_timer as timer

drive.mount('/content/gdrive')

BASE_PATH = os.getcwd() + '/gdrive/My Drive/Colab Notebooks/CIS680/HW3b_Mask'
DATA_PATH = os.path.join(BASE_PATH, 'data')
CHECKPOINT_PATH = os.path.join(BASE_PATH, 'checkpoints')
RANDOM_SEED = 373

np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

"""## Magic Constants (*spooky*) ðŸ‘»ðŸ‘»ðŸ‘»"""

IMAGE_WIDTH = 400
IMAGE_HEIGHT = 300
IMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT) # (Width, Height)

TARGET_WIDTH = 1088
TARGET_HEIGHT = 800
# TARGET_SIZE = (TARGET_WIDTH, TARGET_HEIGHT)
TARGET_SHAPE = (3, TARGET_HEIGHT, TARGET_WIDTH)

RGB_MEAN = [0.485, 0.456, 0.406]
RGB_STD = [0.229, 0.224, 0.225]

# Channel Constants
CH_OBJ = 0
CH_REG = 1

N_CLASSES = 4
PROPOSALS_PER_IMAGE = 50

COLORS = np.array([
                   [0, 0, 0],
                   [255, 0, 0],
                   [0, 0, 255],
                   [0, 255, 0],
                   [125, 125, 125],
                   [50, 125, 50]
], np.uint8)

SHUFFLE_DATASET = True
BATCH_SIZE = 8
TEST_SPLIT = 0.1

"""# Helpers"""

def do_for(fn, mat):
    return np.apply_along_axis(fn, 1, mat)

def exists(it):
    return (it is not None)

def filter_none(it):
    return list(filter(exists, it))

def rev(tup):
    """Reverses a tuple"""
    return tup[::-1]  

def _resize(img, output_shape):
    """Resize a channels-first image with bilinear interpolation to output shape

    Inputs:
        img: a channels first image (CH, H, W)
        output_shape: tuple(W, H)

    Outputs: Resized channels-first image with output shape
    """
    return cv2.resize(
        img.transpose((1,2,0)).astype(np.uint8),
        output_shape,
        interpolation=cv2.INTER_LINEAR).transpose((2,0,1))     

def _resize_mask(mask, output_shape):
    if (len(mask.shape) == 3):
        return cv2.resize(
            mask.transpose((1,2,0)).astype(np.uint8),
            output_shape,
            interpolation=cv2.INTER_NEAREST).transpose((2,0,1))     
    else:
        return cv2.resize(
            mask.astype(np.uint8),
            output_shape,
            interpolation=cv2.INTER_NEAREST)                

def show(img):
    """Show a channels-first image"""
    plt.figure()
    plt.imshow(img.transpose((1,2,0)))

def show_img(dataset, index):
    show(_draw_bounding_boxes(
            dataset.images[index], 
            dataset.bboxes[index],
            dataset.labels[index]))

def show_mask(dataset, index):
    # We need to use mask indices
    masks = dataset.masks[dataset.mask_indices[index-1]:dataset.mask_indices[index]]
    if len(masks.shape) == 2:
        masks = np.expand_dims(masks, 0)

    show(_draw_mask(
            dataset.images[index], 
            masks,
            dataset.labels[index]))

def show_both(dataset, index):
    """Imshow index in dataset with bounding boxes"""
    show_img(dataset, index)
    show_mask(dataset, index)

def _draw_bounding_boxes(image, bboxes, labels):
    """Draw bounding boxes onto channels-first image. Returns copy of image."""
    # ret_copy = image.transpose((1,2,0)).astype(np.uint8).copy()
    ret_copy = image.transpose((1,2,0)).copy()
    for idx in range(labels.shape[0]):
        # Weird fix: https://stackoverflow.com/questions/34932288/how-to-pass-a-numpy-ndarray-as-a-color-in-opencv/47378486
        color = tuple([int(x) for x in COLORS[labels[idx]]]) 
        c1 = tuple(bboxes[idx, :2].astype(int))
        c2 = tuple(bboxes[idx, 2:].astype(int))
        cv2.rectangle(ret_copy, c1, c2, color, 6)

    return ret_copy.transpose((2,0,1))
  
def _draw_mask(image, masks, labels):
    # pdb.set_trace()
    # ret_copy = image.astype(np.uint8).copy()
    ret_copy = image.copy()
    for idx in range(labels.shape[0]):
        color = COLORS[labels[idx]]
        # color = CLASS_COLOR_MAP[labels[idx]]        
        mask = (masks[idx] > 0)
        ret_copy[:, mask] = (np.expand_dims(0.8 * (color / 255.), 1) + (0.2 * ret_copy[:, mask]))

    return ret_copy
    
def unpad_image(img):
    """Removes zero-padding from the boundaries of a channels-first image"""
    return crop(img, unpad_coords(img))

def unpad_coords(img):  
    """Returns the coordinates of the non-padded rectangular region in the image"""
    mask = img > 0
    coords = np.argwhere(mask)[:, 1:] # ignore channel
    y1, x1 = coords.min(axis=0)
    y2, x2 = coords.max(axis=0) + 1
    return x1, y1, x2, y2

def crop(img, coords):
    """Applies a coordinate crop to a channels-first image"""
    x1, y1, x2, y2 = coords
    if len(img.shape) == 3:
        return img[:, y1:y2, x1:x2]
    else:
        return img[y1:y2, x1:x2]

def clip_box_to_pad_coords(box, coords):
    x1, y1, x2, y2 = box
    x1_c, y1_c, x2_c, y2_c = coords
    return x1 - x1_c, y1 - y1_c, x2 - x1_c, y2 - y1_c  

def scale_box(box, ratios):
    x1, y1, x2, y2 = box
    w_ratio, h_ratio = ratios
    return x1 * w_ratio, y1 * h_ratio, x2 * w_ratio, y2 * h_ratio

def _pad_box(box, padding):    
    x1, y1, x2, y2 = box
    top, bottom, left, right = padding[1][0], padding[1][1], padding[2][0], padding[2][1]
    return x1 + left, y1 + top, x2 + left, y2 + top

def _pad_mask(mask, padding):
    if (len(mask.shape) == 2):
        pad_mask = np.pad(np.expand_dims(mask, 0), padding)
    else:
        pad_mask = np.pad(mask, padding)
    return pad_mask

def random_rows_2d(mat, n):
    if isinstance(mat, np.ndarray):
        return mat[np.random.choice(mat.shape[0], n, replace=False), :]
    else:
        idx = np.random.choice(mat.shape[0], n, replace=False)
        return mat[idx, :]

def scale_to_target(image, target_shape):
    """
    Scale channels-last image to target shape

    Returns:
        resize_shape: (out_width, out_height)
        padding: (top, bottom, left, right)
    """
    h_i, w_i = image.shape[1:]
    w_t, h_t = target_shape[1:]

    w_ratio = w_t / w_i
    h_ratio = h_t / h_i

    if w_ratio < h_ratio:
        out_h = int(h_i * w_ratio)
        resize_h = out_h if (out_h % 2 == 0) else (out_h + 1)
        resize_shape = (w_t, resize_h)
        pad_h = h_t - (h_i * w_ratio)
        padding = [(0,0), (pad_h // 2, pad_h // 2),  (0, 0)]
        ratios = (w_ratio, w_ratio)

        if not ((2 * (pad_h // 2) + resize_h) == h_t):
            diff = h_t - (2 * (pad_h // 2) + resize_h)
            padding = [(0,0), ((pad_h // 2 ) + (diff // 2), ( pad_h // 2) + (diff // 2)),  (0, 0)]
    else:
        out_w = int(w_i * h_ratio)
        resize_w = out_w if (out_w % 2 == 0) else (out_w + 1)
        resize_shape = (resize_w, h_t)
        pad_w = w_t - (w_i * h_ratio)
        padding = [(0,0), (0, 0), (pad_w // 2, pad_w // 2)]
        ratios = (h_ratio, h_ratio)

        if not ((2 * (pad_w // 2) + resize_w) == w_t):
            diff = w_t - (2 * (pad_w // 2) + resize_w)
            padding = [(0,0), (0, 0), ((pad_w // 2) + (diff // 2), (pad_w // 2) + (diff // 2))]

    # Padding is [(0, 0), (top, bottom), (left, right)] where (0,0) is channel
    padding = list(map(lambda x: list(map(int, x)), padding))
    return resize_shape, padding, ratios

"""## Box Manipulation"""

def anchor_in_boundary(x1, y1, x2, y2, width, height):
  if ((x1 < 0) or (y1 < 0) or (x2 > width) or (y2 > height)):
    return 0
  else:
    return 1

def filter_boxes_to_in_boundary(boxes, width, height):
    """Filters out boxes that are cross-boundary"""
    in_bounds = torch.tensor([
        anchor_in_boundary(*box, width, height) for box in torch.unbind(boxes, dim=0)
    ]).unsqueeze(-1)

    return (in_bounds == 1).all(1)
    # return boxes[(in_bounds == 1).all(1), :]
    # filter_boxes_to_in_boundary(rpnout[0][0], TARGET_WIDTH, TARGET_HEIGHT)

"""## IoU"""

def area(x1, y1, x2, y2):
  """Return the area of a bounding box"""
  return (x2 - x1) * (y2 - y1)

def get_iou(box1, box2, eps=1e-5):
  """Get the IoU of two bounding boxes"""
  x1_a, y1_a, x2_a, y2_a = box1
  x1_b, y1_b, x2_b, y2_b = box2
  
  box1_area = area(*box1)
  box2_area = area(*box2)
  
  x1 = max(x1_a, x1_b)
  y1 = max(y1_a, y1_b)
  x2 = min(x2_a, x2_b)
  y2 = min(y2_a, y2_b)
   
  if (x2 - x1 < 0.) or (y2 - y1 < 0.):
    overlap_area = 0.
  else:
    overlap_area = area(x1, y1, x2, y2)
    
  combined_area = box1_area + box2_area - overlap_area
  iou = overlap_area / (combined_area + eps)
  return iou

def ratio_iou(x1, y1, x2, y2, x1_a, y1_a, x2_a, y2_a, eps=1e-5):
    xi = torch.max(x1, x1_a)                                 # Intersection
    yi = torch.max(y1, y1_a)
    wi = torch.clamp(torch.min(x1+(x2-x1), x1_a+(x2_a-x1_a))-xi, min=0)
    hi = torch.clamp(torch.min(y1+(y2-y1), y1_a+(y2_a-y1_a))-yi, min=0)
    area_i = wi * hi                                       # Area Intersection
    area_u = (x2-x1) * (y2-y1) + (x2_a-x1_a) * (y2_a-y1_a) - wi * hi                   # Area Union
    return area_i / torch.clamp(area_u, min=eps)

# def align_max_iou_index_to_truth(max_ious, max_indices, num_gt_bboxes, iou_thresh=0.5):
#     """Specifies which bounding box predictions will align with ground truth bounding boxes"""
#     positives = (max_ious > iou_thresh).all(1)
#     # Make sure all-ground truth bounding boxes are actually represented
#     for CL in range(num_gt_bboxes):
#         # None aligned with ground truth box with greater than thresh IoU
#         if (max_indices[positives, :] == CL).sum(dim=0) == 0:
#             # At least one aligned with ground truth box
#             if (max_indices == CL).all(1).sum(dim=0) > 0:
#                 _, idx = max_ious[(max_indices == CL).all(1), :].max(0)
#                 positives[idx] = True

#     return positives.nonzero()

def align_max_iou_index_to_truth_better(max_ious, max_indices, num_gt_bboxes, iou_thresh=0.5):
    """Specifies which bounding box predictions will align with ground truth bounding boxes"""
    positives = (max_ious > iou_thresh)
    # Make sure all-ground truth bounding boxes are actually represented
    for CL in range(num_gt_bboxes):
        # None aligned with ground truth box with greater than thresh IoU
        if (max_indices[positives] == CL).sum(dim=0) == 0:
            # At least one aligned with ground truth box
            if (max_indices == CL).sum(dim=0) > 0:
                _, idx = max_ious[(max_indices == CL)].max(0)
                positives[idx] = True

    return positives.nonzero()

def negative_region_indices(max_ious, max_indices, num_negative, iou_thresh=0.5):
    all_negatives = (max_ious < iou_thresh)
    sampled_negatives = random_rows_2d(all_negatives.nonzero(), num_negative)
    return sampled_negatives

# max_ious, max_indices = max_iou_and_index(rpnout[0][0], rpnout[0][0][:3])
# positives = align_max_iou_index_to_truth(max_ious, max_indices, 3, iou_thresh=0.5)
# negatives = negative_region_indices(max_ious, max_indices, 20)
# print(len(positives))
# print(len(negatives))

"""## Coordinate Transformations"""

def encode_t_coords(x1, y1, x2, y2, x1_a, y1_a, x2_a, y2_a):
    """Inputs are coordinates of ground truth and proposal (anchor)"""
    w, h = (x2 - x1), (y2 - y1)
    x_c, y_c = (x1 + x2) / 2, (y1 + y2) / 2
    
    w_a, h_a = (x2_a - x1_a), (y2_a - y1_a)
    x_c_a, y_c_a = (x1_a + x2_a) / 2, (y1_a + y2_a) / 2

    t_x = (x_c - x_c_a) / w_a
    t_y = (y_c - y_c_a) / h_a
    t_w = torch.log(w / w_a)
    t_h = torch.log(h / h_a)
  
    return torch.cat([t_x, t_y, t_w, t_h], dim=1)

def decode_t_coords(t_x, t_y, t_w, t_h, x1_a, y1_a, x2_a, y2_a):
    """Decodes t-coordinates with respect to proposal (anchor)"""
    w_a, h_a = (x2_a - x1_a), (y2_a - y1_a)
    x_c_a, y_c_a = (x1_a + x2_a) / 2, (y1_a + y2_a) / 2

    x_c = (t_x * w_a) + x_c_a
    y_c = (t_y * h_a) + y_c_a
    w = torch.exp(t_w) * w_a
    h = torch.exp(t_h) * h_a

    x1 = (x_c - (w / 2)).round()
    y1 = (y_c - (h / 2)).round()
    x2 = (x_c + (w / 2)).round()
    y2 = (y_c + (h / 2)).round()

    return torch.cat([x1, y1, x2, y2], dim=1)

# A = torch.tensor([10, 10, 100, 100]).float()
# B = torch.tensor([15, 15, 126, 150]).float()
# t = encode_t_coords(*A, *B)
# print(t)
# A_rec = decode_t_coords(*t, *B)
# print(A_rec)

"""# Dataset"""

def my_collate(batch):
  
  images = torch.stack([torch.from_numpy(sample['images']) for sample in batch])
  
  collated = [images]
  pad_lengths = {}
  
  for field in ['masks', 'bboxes', 'labels']:
    seqs = [torch.from_numpy(sample[field]) for sample in batch]
    packed = rnn.pack_sequence(seqs, enforce_sorted=False)
    padded_seq, lengths = rnn.pad_packed_sequence(packed, batch_first=True, padding_value=-1)
    pad_lengths[field] = lengths
    
    collated.append(padded_seq)
  
  return tuple(collated), pad_lengths


class CocoDataset(torch.utils.data.Dataset):
  def __init__(self, path):
    
    bbox_path = os.path.join(path, 'hw3_mycocodata_bboxes_comp_zlib.npy')
    images_path = os.path.join(path, 'hw3_mycocodata_img_comp_zlib.h5')
    labels_path = os.path.join(path, 'hw3_mycocodata_labels_comp_zlib.npy')
    masks_path = os.path.join(path,  'hw3_mycocodata_mask_comp_zlib.h5')
    
    self.bboxes = np.load(bbox_path, allow_pickle=True, encoding='bytes')
    self.labels = np.load(labels_path, allow_pickle=True, encoding='bytes')
        
    self.masks = h5py.File(masks_path, 'r')['data']    
    self.images = h5py.File(images_path, 'r')['data']
    
    self.mask_indices = np.cumsum([len(l) for l in self.labels])
    
  def __len__(self):
    return self.labels.shape[0]
   
  def __getitem__(self, idx):
    if torch.is_tensor(idx):
      idx = idx.tolist()
    
    # Ensures masks are provided correctly
    if idx == 0:
      masks = self.masks[:self.mask_indices[0]]
    else:
      masks = self.masks[self.mask_indices[idx-1]:self.mask_indices[idx]]
      
    masks = masks.squeeze()

    upd_coords = unpad_coords(self.images[idx])
    upd_img = unpad_image(self.images[idx])
    upd_mask = crop(masks, upd_coords)
    
    resize_shape, padding, ratios = scale_to_target(upd_img, (3, TARGET_WIDTH, TARGET_HEIGHT))
    
    resized_img = _resize(upd_img, resize_shape)
    resized_mask = _resize_mask(upd_mask, resize_shape)

    resize_bboxes = do_for(
        lambda r: _pad_box(scale_box(clip_box_to_pad_coords(r, upd_coords), ratios), padding),
        self.bboxes[idx])

    pad_mask = _pad_mask(resized_mask, padding)
    pad_img = np.pad(resized_img, padding)
    batch = {
        'index': idx,
        'images': (((pad_img.astype(np.float32) / 255.0).transpose((1,2,0)) - RGB_MEAN) / RGB_STD).transpose((2,0,1)),
        # 'images': pad_img.astype(np.int32),
        'masks': pad_mask.astype(np.int32),
        'labels': self.labels[idx],
        'bboxes': resize_bboxes
    }

    return batch

"""### Load the Dataset"""

############################################
# Load the dataset
ds = CocoDataset(DATA_PATH)
DATASET_SIZE = len(ds)

# Creating data indices for training and validation splits:
indices = list(range(DATASET_SIZE))
split = int(np.floor(TEST_SPLIT * DATASET_SIZE))

if SHUFFLE_DATASET :
    np.random.seed(RANDOM_SEED)
    np.random.shuffle(indices)
    
train_indices, test_indices = indices[split:], indices[:split]
tiny_indices = indices[:16]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
test_sampler = SubsetRandomSampler(test_indices)
tiny_sampler = SubsetRandomSampler(tiny_indices)

train_loader = DataLoader(
    ds, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=my_collate)

test_loader = DataLoader(
    ds, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=my_collate)

tiny_loader = DataLoader(
    ds, batch_size=2, sampler=tiny_sampler, collate_fn=my_collate)
############################################

"""### Check Dataset"""

batch = ds.__getitem__(29)
box_img = _draw_bounding_boxes(batch["images"], batch["bboxes"], batch["labels"])
show(_draw_mask(box_img, batch["masks"], batch["labels"]))

"""# RPN Ground Truth"""

def sample_ground_truth(proposals, gt_bboxes, gt_labels, iou_thresh=0.5):        
    """
    Inputs:
        pred_bboxes: torch.tensor((1000, 4))
        truth_bboxes: torch.tensor((N, 4))
        labels: torch.tensor(N)
    # """
    
    # For every ground truth box
    scores = []
    for pidx in range(gt_bboxes.shape[0]):
        # IoU with every proposal
        scores.append(
            ratio_iou(*torch.cat([
                 gt_bboxes[pidx].expand((proposals.shape[0], -1)),
                 proposals
                 ], dim=1).split(1, dim=1)))
        
    scores = torch.cat(scores, dim=1)
    ious, indices = torch.max(scores, dim=1)
    
    positive_inds = align_max_iou_index_to_truth_better(
        ious, indices, gt_bboxes.shape[0], iou_thresh=iou_thresh)

    # Cap the number of positive regions per image at PROPOSALS_PER_IMAGE
    if len(positive_inds) > PROPOSALS_PER_IMAGE:
        positive_inds = random_rows_2d(positive_inds, PROPOSALS_PER_IMAGE)
        
    # Get the positives
    positive_proposals = proposals[positive_inds.squeeze(1)]
    positive_bboxes = gt_bboxes[indices[positive_inds.squeeze(1)]]
    positive_labels = gt_labels[indices[positive_inds.squeeze(1)]].long()

    n_positive = len(positive_inds)
    n_negative = PROPOSALS_PER_IMAGE - n_positive

    if n_negative > 0:
        negative_inds = negative_region_indices(ious, indices, n_negative)
        negative_proposals = proposals[negative_inds.squeeze(1)]
        # Note: for negative ground truth, we use the proposals themselves.
        # This is so that the loss is zero for negative proposals 
        negative_bboxes = proposals[negative_inds.squeeze(1)]
        negative_labels = torch.zeros_like(negative_inds.squeeze(1)).long()

        return (positive_proposals,
                positive_bboxes,
                positive_labels,
                negative_proposals,
                negative_bboxes,
                negative_labels)
    else:
        return (positive_proposals,
                positive_bboxes, 
                positive_labels, 
                None, None, None)

"""# Pretrained Model"""

def pretrained_model_680(checkpoint_file):
    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False)
    model.eval()
    model.to(DEVICE)

    backbone = model.backbone
    rpn = model.rpn
    
    checkpoint = torch.load(checkpoint_file)

    backbone.load_state_dict(checkpoint['backbone'])
    rpn.load_state_dict(checkpoint['rpn'])
    
    return backbone, rpn

class ROIAlign(torch.nn.Module):
    def __init__(self, P, n_feature_maps=1):
        super(ROIAlign, self).__init__()
        self.P = P
        self.flatten = nn.Flatten(start_dim=1)
        self.linear1 = nn.Linear(n_feature_maps * 256 * P * P, 1024)
        self.linear2 = nn.Linear(1024, 1024)

    def forward(self, X):
        X = self.flatten(X)
        X = F.relu(self.linear1(X))
        X = F.relu(self.linear2(X))
        return X

class Classifier(torch.nn.Module):
  def __init__(self):
    super(Classifier, self).__init__()
    self.linear1 = nn.Linear(1024, N_CLASSES)

  def forward(self, X):
      X = self.linear1(X)
      # OBSERVE NO SOFTMAX - CrossEntropyLoss will build-in LogSoftmax
      return X

class Regressor(torch.nn.Module):
  def __init__(self):
    super(Regressor, self).__init__()
    self.linear1 = nn.Linear(1024, N_CLASSES * 4)

  def forward(self, X):
    return self.linear1(X)

class Masks(torch.nn.Module):
  def __init__(self):
    super(Masks, self).__init__()
    self.conv1 = nn.Conv2d(256, 256, 3, stride=1, padding=3//2)
    self.conv2 = nn.Conv2d(256, 256, 3, stride=1, padding=3//2)
    self.conv3 = nn.Conv2d(256, 256, 3, stride=1, padding=3//2)
    self.conv4 = nn.Conv2d(256, 256, 3, stride=1, padding=3//2)
    self.conv5 = nn.Conv2d(256, 4, 1, stride=1)        
    self.conv1_tr = nn.ConvTranspose2d(256, 256, 2, stride=2)
    
  def forward(self, X):
    X = F.relu(self.conv1(X))
    X = F.relu(self.conv2(X))
    X = F.relu(self.conv3(X))
    X = F.relu(self.conv4(X))
    X = F.relu(self.conv1_tr(X))
    X = self.conv5(X)
    return X

"""# Training"""

BACKBONE, RPN = pretrained_model_680(os.path.join(CHECKPOINT_PATH, 'checkpoint680.pth'))

BACKBONE.eval()
RPN.eval()

for params in BACKBONE.parameters():
    params.requires_grad = False

for params in RPN.parameters():
    params.requires_grad = False

USE_PRETRAINED = True
FREEZE_BOX_HEAD = False

ROI_NET = ROIAlign(P=7, n_feature_maps=1).to(DEVICE)
CLASS_NET = Classifier().to(DEVICE)
REG_NET = Regressor().to(DEVICE)
MASK_NET = Masks().to(DEVICE)

CLLoss = nn.CrossEntropyLoss()
SmoothLoss = nn.SmoothL1Loss()
BCELoss = nn.BCELoss()

print("Instantiated Sub-networks")

if USE_PRETRAINED:
    try:
        ROI_NET.load_state_dict(torch.load(os.path.join(CHECKPOINT_PATH, 'ROI_2019-11-20_02:23:42.ckpt')))
        CLASS_NET.load_state_dict(torch.load(os.path.join(CHECKPOINT_PATH, 'Classifier_2019-11-20_02:23:42.ckpt')))
        REG_NET.load_state_dict(torch.load(os.path.join(CHECKPOINT_PATH, 'Regressor_2019-11-20_02:23:42.ckpt')))      
        print("Loaded weights for all intermediate networks")
    except:
        print("One of the checkpoints was not found")

if FREEZE_BOX_HEAD:
    for params in ROI_NET.parameters():
        params.requires_grad = False
    for params in CLASS_NET.parameters():
        params.requires_grad = False
    for params in REG_NET.parameters():
        params.requires_grad = False        
    print("Box head frozen")        

# Optional debugging summary
# summary(ROI_NET, (256, 7, 7))
# summary(REG_NET, (1024,))
# summary(CLASS_NET, (1024,))
# summary(MASK_NET, (256, 14, 14))

"""## Training Loop"""

torch.cuda.empty_cache()

EPOCHS = 1
LOG_LOSS_MINI_BATCHES = 20
SAVE_EPOCH_CHECKPOINTS = True

# Accumulated Proposal classifier losses
train_proposal_total = []
train_regressor_total = []
train_mask_total = []

# Optimizer with scheduler
params = list(ROI_NET.parameters()) + list(CLASS_NET.parameters()) + list(REG_NET.parameters()) + list(MASK_NET.parameters())
optimizer = torch.optim.Adam(params, lr=2e-4)
torch.optim.lr_scheduler.StepLR(optimizer, 3, gamma=0.1)

for epoch in range(EPOCHS):

    # Inference losses will be set created here
    running_prop, running_reg, running_mask = 0.0, 0.0, 0.0

    for idx, (batch, pad_lengths) in enumerate(tiny_loader):

        # zero the parameter gradients
        optimizer.zero_grad()

        images, masks, bboxes, labels = batch

        images = images.to(device=DEVICE, dtype=torch.float)
        bboxes = bboxes.to(device=DEVICE, dtype=torch.float)
        labels = labels.to(device=DEVICE, dtype=torch.float)
        masks = masks.to(device=DEVICE, dtype=torch.float)
        
        backbone_out = BACKBONE(images)

        img_list = ImageList(
            images, list(itertools.repeat((TARGET_HEIGHT, TARGET_WIDTH), len(images))))

        rpn_proposals = RPN(img_list, backbone_out)[0]

        sel_pos_proposals = [1]*len(images)
        sel_pos_bboxes = [1]*len(images)
        sel_pos_labels = [1]*len(images)

        POS_CT = [1]*len(images)

        sel_neg_proposals = [1]*len(images)
        sel_neg_bboxes = [1]*len(images)
        sel_neg_labels = [1]*len(images)

        # Sample rpn proposals for positive and negative proposals
        for ix, proposals in enumerate(rpn_proposals):
            ground_truth = sample_ground_truth(
                proposals,
                bboxes[ix][:pad_lengths["bboxes"][ix]],
                labels[ix][:pad_lengths["labels"][ix]],
                iou_thresh=0.5)

            positive_proposals, positive_bboxes, positive_labels, negative_proposals, negative_bboxes, negative_labels = ground_truth
            
            # Positive samples
            sel_pos_proposals[ix] = positive_proposals
            sel_pos_bboxes[ix] = positive_bboxes
            sel_pos_labels[ix] = positive_labels
            # sel_pos_masks[ix] = positive_masks
            POS_CT[ix] = len(positive_labels)

            # Negatives samples
            sel_neg_proposals[ix] = negative_proposals
            sel_neg_bboxes[ix] = negative_bboxes
            sel_neg_labels[ix] = negative_labels

        sel_proposals = sel_pos_proposals + filter_none(sel_neg_proposals)
        sel_bboxes = sel_pos_bboxes + filter_none(sel_neg_bboxes)
        sel_labels = sel_pos_labels + filter_none(sel_neg_labels)

        roi_aligned_proposals = torchvision.ops.roi_align(
            backbone_out[0],
            sel_proposals,
            (7,7),
            spatial_scale=1./4.,
            sampling_ratio=4)

        sel_proposals = torch.cat(sel_proposals, dim=0)
        sel_bboxes = torch.cat(sel_bboxes, dim=0)
        sel_labels = torch.cat(sel_labels, dim=0)
    
        sel_pos_proposals = torch.cat(sel_pos_proposals, dim=0)
        sel_pos_bboxes = torch.cat(sel_pos_bboxes, dim=0)
        sel_pos_labels = torch.cat(sel_pos_labels, dim=0)
        
        # Total num. of positive proposals
        n_pos_proposals = len(sel_pos_proposals) 
        print(n_pos_proposals)

        pdb.set_trace()
        # Roi Aligned into Intermediate then Regressor/Classifier
        roi_out = ROI_NET(roi_aligned_proposals)
        class_out = CLASS_NET(roi_out)
        reg_out = REG_NET(roi_out)

        # Isolate the selected bbox from prediction probabilities
        pos_regressor_proposals = torch.gather(
            reg_out[:n_pos_proposals], 1, 
            torch.stack([torch.arange(l*4,((l+1)*4)) for l in sel_pos_labels]).cuda())

        # Ground truth bounding boxes encoded in t-coordinates
        pos_t_encoded_bboxes = encode_t_coords(*torch.cat(
            [sel_pos_bboxes, sel_pos_proposals], dim=1).split(1, dim=1))       

        # Box Head Losses
        class_loss = CLLoss(class_out, sel_labels)
        reg_loss = SmoothLoss(pos_regressor_proposals, pos_t_encoded_bboxes)

        # THIS IS ALL FUCKED, FIX FROM HERE

        pos_pred_probs, pos_pred_classes = torch.softmax(class_out[:n_pos_proposals], dim=1).max(dim=1)
        print(len(pos_pred_probs), len(pos_pred_classes))

        # regressor_proposals = torch.gather(
        #     reg_out, 1, 
        #     torch.stack([torch.arange(l*4,((l+1)*4)) for l in sel_labels]).cuda())

        # Decode from t-coordinates
        # decoded_regressor_proposals = decode_t_coords(*torch.cat(
        #     [regressor_proposals, sel_proposals], dim=1).split(1, dim=1))

        pos_decoded_regressor_proposals = decode_t_coords(*torch.cat(
            [pos_regressor_proposals, sel_pos_proposals], dim=1).split(1, dim=1))
        
        print(pos_decoded_regressor_proposals.shape)

        # Re-split into list of tensors of positive regressor proposals
        pos_decoded_regressor_proposals = torch.split(pos_decoded_regressor_proposals, POS_CT, dim=0)
        pos_pred_classes = torch.split(pos_pred_classes, POS_CT, dim=0)
        pos_pred_probs = torch.split(pos_pred_probs, POS_CT, dim=0)
        sel_pos_labels = torch.split(sel_pos_labels, POS_CT, dim=0)
        sel_pos_bboxes = torch.split(sel_pos_bboxes, POS_CT, dim=0)

        nms_filtered_proposals, nms_filtered_bboxes, nms_filtered_pred_probs, nms_filtered_pred_labels, nms_filtered_masks, nms_filtered_true_labels = [], [], [], [], [], []
        for rx in range(len(images)):

            img_proposals = pos_decoded_regressor_proposals[rx]
            img_pred_probs = pos_pred_probs[rx]
            img_pred_classes = pos_pred_classes[rx]
            img_masks = sel_pos_masks[rx]
            img_labels = sel_pos_labels[rx]
            img_bboxes = sel_pos_bboxes[rx]

            # Filter invalid bboxes
            keep_indices = filter_boxes_to_in_boundary(
                img_proposals, TARGET_WIDTH, TARGET_HEIGHT)
        
            img_proposals = img_proposals[keep_indices, :]
            img_pred_probs = img_pred_probs[keep_indices]
            img_pred_classes = img_pred_classes[keep_indices]
            img_masks = img_masks[keep_indices, :, :]
            img_labels = img_labels[keep_indices]
            img_bboxes = img_bboxes[keep_indices, :]

            # Non-negative Maximum Suppression over non-background classes (NMS)
            nms_kept_indices = []
            for ix in range(1, N_CLASSES):
                if (img_pred_classes == ix).sum(0) > 0:
                    keep_indices = torchvision.ops.nms(
                        img_proposals[(img_pred_classes == ix), :],
                        img_pred_probs[(img_pred_classes == ix)],
                        iou_threshold=0.7)
                    
                    if len(keep_indices) > 0:
                        nms_kept_indices.append(
                            (img_pred_classes == ix).nonzero()[keep_indices])
            
            if len(nms_kept_indices) > 0:
                nms_kept_indices = torch.cat(nms_kept_indices, dim=0).squeeze(1)
                nms_filtered_proposals.append(img_proposals[nms_kept_indices, :])
                nms_filtered_pred_probs.append(img_pred_probs[nms_kept_indices])
                nms_filtered_pred_labels.append(img_pred_classes[nms_kept_indices])
                nms_filtered_masks.append(img_masks[nms_kept_indices, :, :])
                nms_filtered_true_labels.append(img_labels[nms_kept_indices])
                nms_filtered_bboxes.append(img_bboxes[nms_kept_indices, :])

        nms_filtered_proposals = torch.cat(nms_filtered_proposals, dim=0)
        nms_filtered_pred_probs = torch.cat(nms_filtered_pred_probs, dim=0)
        nms_filtered_pred_labels = torch.cat(nms_filtered_pred_labels, dim=0).long()
        nms_filtered_masks = torch.cat(nms_filtered_masks, dim=0)
        nms_filtered_true_labels = torch.cat(nms_filtered_true_labels, dim=0).long()
        nms_filtered_bboxes = torch.cat(nms_filtered_bboxes, dim=0)
    
        # Use ROIAlign as helper to crop and resize mask based on ground truth bounding box - i.e. ground truth mask
        nms_filtered_masks = torch.cat([
                torchvision.ops.roi_align(
                    mask.unsqueeze(0).unsqueeze(0),
                    [box.unsqueeze(0)], (28,28)) for box, mask in zip(
                        nms_filtered_bboxes.split(1),
                        nms_filtered_masks.split(1))], dim=0).squeeze(1)

        # ROI Align proposals to create "detections"
        roi_aligned_detections = torchvision.ops.roi_align(
            backbone_out[0],
            [nms_filtered_proposals],
            (14,14),
            spatial_scale=1./4.,
            sampling_ratio=4)

        mask_out = MASK_NET(roi_aligned_detections)
        
        pred_masks = torch.sigmoid(
            torch.stack([mask_out[ix, l] for ix, l in enumerate(nms_filtered_true_labels)]))

        mask_loss = BCELoss(pred_masks, nms_filtered_masks)

        total_loss = reg_loss + (2. * class_loss) + mask_loss

        # Backpropogate on total class/regress network loss
        total_loss.backward()
        optimizer.step()

        # Track losses
        running_prop += class_loss.item()
        running_reg += reg_loss.item()
        running_mask += mask_loss.item()

        if idx % LOG_LOSS_MINI_BATCHES == (LOG_LOSS_MINI_BATCHES - 1):   
            # print(timer() - start)             
            print("Epoch: {}, Batch: {} | Prop: {:.3f}| Reg {:.3f} | Mask: {:.3f}".format(
                epoch + 1, idx + 1,
                running_prop / LOG_LOSS_MINI_BATCHES,
                running_reg / LOG_LOSS_MINI_BATCHES,
                running_mask / LOG_LOSS_MINI_BATCHES
            ))
            train_proposal_total.append(running_prop / LOG_LOSS_MINI_BATCHES)
            train_regressor_total.append(running_reg / LOG_LOSS_MINI_BATCHES)
            train_mask_total.append(running_mask / LOG_LOSS_MINI_BATCHES)            

            running_prop, running_reg, running_mask = 0.0, 0.0, 0.0
    
    if SAVE_EPOCH_CHECKPOINTS:
        checkpoint_file = datetime.now().strftime("ROI_%Y-%m-%d_%H:%M:%S.ckpt")
        torch.save(ROI_NET.state_dict(), os.path.join(CHECKPOINT_PATH, checkpoint_file))
        checkpoint_file = datetime.now().strftime("Classifier_%Y-%m-%d_%H:%M:%S.ckpt")
        torch.save(CLASS_NET.state_dict(), os.path.join(CHECKPOINT_PATH, checkpoint_file))
        checkpoint_file = datetime.now().strftime("Regressor_%Y-%m-%d_%H:%M:%S.ckpt")
        torch.save(REG_NET.state_dict(), os.path.join(CHECKPOINT_PATH, checkpoint_file))
        checkpoint_file = datetime.now().strftime("Mask_%Y-%m-%d_%H:%M:%S.ckpt")
        torch.save(MASK_NET.state_dict(), os.path.join(CHECKPOINT_PATH, checkpoint_file))

"""### Plot Classifier/Regressor Train Losses"""

fig = plt.figure()
ax = plt.subplot(111)

plt.title("Proposal Classifier Training Loss (3 Epochs)")
plt.xlabel("20 Mini-batch Interval")
plt.ylabel("Mean loss / sample")
ax.plot(train_proposal_total, label="Proposal classifier train loss")
ax.legend( prop={'size': 12}, framealpha=1.0)
ax.grid(which='major')

# plt.savefig("classifier_training_loss.png", dpi=600)
# files.download("classifier_training_loss.png") 

fig = plt.figure()
ax = plt.subplot(111)

plt.title("Proposal Regressor Training Loss (3 Epochs)")
plt.xlabel("20 Mini-batche Interval")
plt.ylabel("Mean loss / sample")
ax.plot(train_regressor_total, label="Proposal regressor train loss")
ax.legend( prop={'size': 12}, framealpha=1.0)
ax.grid(which='major')

# import matplotlib.ticker as ticker
# tick_spacing = 1
# ax.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))

# plt.savefig("regressor_training_loss.png", dpi=600)
# files.download("regressor_training_loss.png") 

# with open('classifier_training_loss', 'w') as file_handler:
#     for item in train_proposal_total:
#         file_handler.write("{}\n".format(item))

# files.download("classifier_training_loss") 

# with open('regressor_training_loss', 'w') as file_handler:
#     for item in train_regressor_total:
#         file_handler.write("{}\n".format(item))

# files.download("regressor_training_loss")

"""## Proposal Classifier Confusion Matrix"""

def proposal_confusion_matrix(loader):
    """Returns mean loss per sample in loader"""
        
    TP, FP, TN, FN = 0, 0, 0, 0

    with torch.no_grad():

        for idx, (batch, pad_lengths) in enumerate(loader):

            images, masks, bboxes, labels = batch

            images = images.to(device=DEVICE, dtype=torch.float)
            bboxes = bboxes.to(device=DEVICE, dtype=torch.float)
            labels = labels.to(device=DEVICE, dtype=torch.float)
            
            backbone_out = BACKBONE(images)

            img_list = ImageList(
                images, list(itertools.repeat((TARGET_HEIGHT, TARGET_WIDTH), len(images))))

            rpn_proposals = RPN(img_list, backbone_out)[0]

            sel_pos_proposals = [1]*len(images)
            sel_pos_bboxes = [1]*len(images)
            sel_pos_labels = [1]*len(images)

            sel_neg_proposals = [1]*len(images)
            sel_neg_bboxes = [1]*len(images)
            sel_neg_labels = [1]*len(images)
            
            # Sample rpn proposals for positive and negative proposals
            for ix, proposals in enumerate(rpn_proposals):
                ground_truth = sample_ground_truth(
                    proposals,
                    bboxes[ix][:pad_lengths["bboxes"][ix]],
                    labels[ix][:pad_lengths["labels"][ix]],
                    iou_thresh=0.5)

                positive_proposals, positive_bboxes, positive_labels, negative_proposals, negative_bboxes, negative_labels = ground_truth
                
                # Positive samples
                sel_pos_proposals[ix] = positive_proposals
                sel_pos_bboxes[ix] = positive_bboxes
                sel_pos_labels[ix] = positive_labels
                
                # Negatives samples
                sel_neg_proposals[ix] = negative_proposals
                sel_neg_bboxes[ix] = negative_bboxes
                sel_neg_labels[ix] = negative_labels

            sel_proposals = sel_pos_proposals + filter_none(sel_neg_proposals)
            sel_bboxes = sel_pos_bboxes + filter_none(sel_neg_bboxes)
            sel_labels = sel_pos_labels + filter_none(sel_neg_labels)

            # ROI Align
            roi_aligned_proposals = torchvision.ops.roi_align(
                backbone_out[0],
                sel_proposals,
                (7,7),
                spatial_scale=1./4.,
                sampling_ratio=4)

            sel_proposals = torch.cat(sel_proposals, dim=0)
            sel_bboxes = torch.cat(sel_bboxes, dim=0)
            sel_labels = torch.cat(sel_labels, dim=0)
        
            sel_pos_proposals = torch.cat(sel_pos_proposals, dim=0)
            sel_pos_bboxes = torch.cat(sel_pos_bboxes, dim=0)
            sel_pos_labels = torch.cat(sel_pos_labels, dim=0)
            
            # the total num. of positive proposals
            n_pos_proposals = len(sel_pos_proposals) 

            # Roi Aligned into Intermediate then Regressor/Classifier
            roi_out = ROI_NET(roi_aligned_proposals)
            class_out = CLASS_NET(roi_out)

            pred_probs, pred_classes = torch.softmax(class_out, dim=1).max(dim=1)

            tp = (pred_classes[(sel_labels != 0).nonzero().squeeze()] == sel_labels[(sel_labels != 0).nonzero().squeeze()]).sum(0).item()
            fp = (pred_classes != 0).sum(0).item() - tp

            tn = (pred_classes[(sel_labels == 0).nonzero().squeeze()] == sel_labels[(sel_labels == 0).nonzero().squeeze()]).sum(0).item()
            fn = (pred_classes == 0).sum(0).item() - tn

            TP += tp
            FP += fp
            TN += tn
            FN += fn

    accuracy = (TP + TN) / (TP + FP + TN + FN)
    precision = TP / (TP + FP)
    recall = TP / (TP + FN)
    
    return accuracy, precision, recall

## UNCOMMENT IF YOU ACTUALLY WANT TO RUN THIS!

# accuracy, precision, recall = proposal_confusion_matrix(test_loader)
# print(accuracy, precision, recall)

"""# Visualizing Box Head Output ðŸ“·"""

TEST_SAMPLE = 55
with torch.no_grad():

    batch = ds.__getitem__(test_indices[TEST_SAMPLE])

    images = torch.from_numpy(batch["images"]).to(device=DEVICE, dtype=torch.float).unsqueeze(0)
    bboxes = torch.from_numpy(batch["bboxes"]).to(device=DEVICE, dtype=torch.float)
    labels = torch.from_numpy(batch["labels"]).to(device=DEVICE, dtype=torch.float)
    masks = torch.from_numpy(batch["masks"]).to(device=DEVICE, dtype=torch.float).unsqueeze(0)

    backbone_out = BACKBONE(images)

    img_list = ImageList(
        images, list(itertools.repeat((TARGET_HEIGHT, TARGET_WIDTH), len(images))))

    rpn_proposals = RPN(img_list, backbone_out)[0][0]

    # ROI Align
    roi_aligned_proposals = torchvision.ops.roi_align(
        backbone_out[1],
        [rpn_proposals],
        (7,7),
        spatial_scale=1./8.,
        sampling_ratio=4)

    roi_out = ROI_NET(roi_aligned_proposals)
    class_out = CLASS_NET(roi_out)
    reg_out = REG_NET(roi_out)

    pred_probs, pred_classes = torch.softmax(class_out, dim=1).max(dim=1)

    # Isolate regressor bounding box from predicted class
    regressor_bboxes = torch.gather(
        reg_out, 1, 
        torch.stack([torch.arange(l*4,((l+1)*4)) for l in pred_classes]).cuda())

    # Decode from t-coordinates
    decoded_regressor_bboxes = decode_t_coords(*torch.cat(
        [regressor_bboxes, rpn_proposals], dim=1).split(1, dim=1))

    # Filter invalid bboxes
    keep_indices = filter_boxes_to_in_boundary(
        decoded_regressor_bboxes, TARGET_WIDTH, TARGET_HEIGHT)
    
    decoded_regressor_bboxes = decoded_regressor_bboxes[keep_indices, :]
    pred_probs = pred_probs[keep_indices]
    pred_classes = pred_classes[keep_indices]
    # rpn_proposals = rpn_proposals[keep_indices, :]

    # Non-negative Maximum Suppression over non-background classes (NMS)
    nms_kept_indices = []
    for ix in range(1, N_CLASSES):
        if (pred_classes == ix).sum(0) > 0:
            keep_indices = torchvision.ops.nms(
                decoded_regressor_bboxes[(pred_classes == ix), :],
                pred_probs[(pred_classes == ix)],
                iou_threshold=0.5)
            
            nms_kept_indices.append((pred_classes == ix).nonzero()[keep_indices])
    
    nms_kept_indices = torch.cat(nms_kept_indices, dim=0).squeeze(1)

    decoded_regressor_bboxes = decoded_regressor_bboxes[nms_kept_indices, :]
    pred_probs = pred_probs[nms_kept_indices]
    pred_classes = pred_classes[nms_kept_indices]
    # rpn_proposals = rpn_proposals[nms_kept_indices, :]

    # Sort globally by prediction probability
    sort_indices = torch.argsort(pred_probs, dim=0, descending=True)
    decoded_regressor_bboxes = decoded_regressor_bboxes[sort_indices, :]
    pred_probs = pred_probs[sort_indices]
    pred_classes = pred_classes[sort_indices]
    # rpn_proposals = rpn_proposals[sort_indices, :]

    # Select the top "N" proposals to visualize - N=5
    top_10_bboxes = decoded_regressor_bboxes[:10]
    top_10_labels = pred_classes[:10]
    
    #### Ablation check for regressor effectiveness

    # ablation_boxes = torch.cat(
    #     [decoded_regressor_bboxes[:1], rpn_proposals[:1]], dim=0)

    # ablation_labels = np.array([1,2], dtype=np.int64)

show(_draw_bounding_boxes(
    batch["images"],
    top_10_bboxes.detach().cpu().numpy(),
    top_10_labels.detach().cpu().numpy()))

# plt.savefig("proposals_{}.png".format(TEST_SAMPLE), dpi=600)
# files.download("proposals_{}.png".format(TEST_SAMPLE)) 

show(_draw_bounding_boxes(
    batch["images"],
    batch["bboxes"],
     batch["labels"]))

# plt.savefig("ground_truth_{}.png".format(TEST_SAMPLE), dpi=600)
# files.download("ground_truth_{}.png".format(TEST_SAMPLE)) 

# show(_draw_bounding_boxes(
#     batch["images"],
#     ablation_boxes.detach().cpu().numpy(),
#     ablation_labels))

# plt.savefig("ablation_{}.png".format(TEST_SAMPLE), dpi=600)
# files.download("ablation_{}.png".format(TEST_SAMPLE))

"""### Visualizing RPN Proposals"""

show(_draw_bounding_boxes(
    batch["images"],
    random_rows_2d(rpn_proposals, 10).detach().cpu().numpy(),
    3 * np.ones((10), dtype=np.int64)))

# show(_draw_bounding_boxes(
#     batch["images"],
#     random_rows_2d(roi_aligned_proposals, 10).detach().cpu().numpy(),
#     1 * np.ones((10), dtype=np.int64)))

# plt.savefig("original_image.png", dpi=600)
# files.download("original_image.png") 

# plt.savefig("rpn_proposals.png", dpi=600)
# files.download("rpn_proposals.png")

grid_img = torchvision.utils.make_grid(
    torch.flatten(roi_aligned_proposals, start_dim=0, end_dim=1)[:150].unsqueeze(1), nrow=15)

plt.imshow(grid_img.permute(1, 2, 0).cpu())

# plt.savefig("roi_align_proposals.png", dpi=600)
# files.download("roi_align_proposals.png")

"""# Misc"""

!ps -aux|grep python

!/opt/bin/nvidia-smi

torch.cuda.empty_cache()

len(train_loader)



# Regressor Loss
        # reg_loss = 0.
        # for ix, positive_inds in enumerate(sel_positive_inds):
        #     pdb.set_trace()
        #     if len(positive_inds) > 0:
        #         # Regressor outputted bbox corresponding to the true label
        #         regressor_bboxes = torch.gather(
        #             reg_out[ix],
        #             1, 
        #             torch.stack([torch.arange(l*4,((l+1)*4)) for l in sel_labels[ix]]).cuda())
        #         pdb.set_trace()
        #         # Regression loss only on positive proposals
        #         reg_loss += SmoothLoss(
        #             regressor_bboxes[positive_inds.squeeze(1)],
        #             t_encoded_bboxes[ix][positive_inds.squeeze(1)])

"""Alternating training of RPN heads vs training jointly. In alternating you lose the gradient from the output to the RPN - you can only. CITE THE PAPER - it's in there. Search the one "joint".

<!-- It is not possible to  -->

In theory, yes. The hierarchical representation of the person might be possible. As in, the regressor has the computational power with sufficient depth to learn how to expand the box, but somewhat dataset dependent and hasn't actually been shown. Like given a bunch of ground truths of torsos and 

Why do we fine tune? To improve the model pre-trainined on COCO. It's not like our mini-dataset on 3 classes is IID. Let's say the model pre-trainined on COCO is really good with very low error, but has minor error for specific classes, then now that accumulates now that we're training on specific classes.
"""

