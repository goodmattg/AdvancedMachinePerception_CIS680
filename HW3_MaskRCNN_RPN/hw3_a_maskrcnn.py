# -*- coding: utf-8 -*-
"""HW3_a_MaskRCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GX2clNIR7U90YlqGaWm4AMAX06WVd_VE
"""

import torch, torchvision, cv2, os

import numpy as np
import torch.nn as nn
import torch.nn.utils.rnn as rnn
import torch.optim as optim
import pdb
import itertools, collections

import matplotlib.pyplot as plt

from datetime import datetime

from torchsummary import summary

from torch.utils.data import DataLoader
from torch.utils.data.sampler import SubsetRandomSampler
from torchvision.utils import make_grid

import h5py

from google.colab import files
from google.colab import drive
drive.mount('/content/gdrive')

BASE_PATH = os.getcwd() + '/gdrive/My Drive/Colab Notebooks/CIS680/HW3'
DATA_PATH = os.path.join(BASE_PATH, 'data')
CHECKPOINT_PATH = os.path.join(BASE_PATH, 'checkpoints')
RANDOM_SEED = 777

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

"""# Dataset"""

def my_collate(batch):
  
  images = torch.stack([torch.from_numpy(sample['images']) for sample in batch])
  truth = torch.stack([torch.from_numpy(sample['truth']) for sample in batch])
  
  collated = [images, truth]
  pad_lengths = {}
  
  for field in ['masks', 'bboxes', 'labels']:
    seqs = [torch.from_numpy(sample[field]) for sample in batch]
    packed = rnn.pack_sequence(seqs, enforce_sorted=False)
    padded_seq, lengths = rnn.pad_packed_sequence(packed, batch_first=True, padding_value=-1)
    pad_lengths[field] = lengths
    
    collated.append(padded_seq)
  
  return tuple(collated), pad_lengths


class HW3Dataset(torch.utils.data.Dataset):
  def __init__(self, path):
    
    bbox_path = os.path.join(path, 'hw3_mycocodata_bboxes_comp_zlib.npy')
    images_path = os.path.join(path, 'hw3_mycocodata_img_comp_zlib.h5')
    labels_path = os.path.join(path, 'hw3_mycocodata_labels_comp_zlib.npy')
    masks_path = os.path.join(path,  'hw3_mycocodata_mask_comp_zlib.h5')
    truth_path = os.path.join(path, 'rpn_ground_truth.npy')
    
    self.bboxes = np.load(bbox_path, allow_pickle=True, encoding='bytes')
    self.labels = np.load(labels_path, allow_pickle=True, encoding='bytes')
    self.truth = np.load(truth_path, allow_pickle=True, encoding='bytes')
        
    self.masks = h5py.File(masks_path, 'r')['data']    
    self.images = h5py.File(images_path, 'r')['data']
    
    self.mask_indices = np.cumsum([len(l) for l in self.labels])
    
  def __len__(self):
    return self.labels.shape[0]
   
  def __getitem__(self, idx):
    if torch.is_tensor(idx):
      idx = idx.tolist()
    
    # Ensures masks are provided correctly
    if idx == 0:
      masks = self.masks[:self.mask_indices[0]]
    else:
      masks = self.masks[self.mask_indices[idx-1]:self.mask_indices[idx]]
      
    batch = {
        'index': idx,
        'images': self.images[idx].astype(np.int32),
        'masks': masks.astype(np.int32),
        'truth': self.truth[idx],
        'labels': self.labels[idx],
        'bboxes': self.bboxes[idx]
    }
    
    return batch

############################################
# MAGIC CONSTANTS DO NOT TOUCH 
############################################
IMAGE_WIDTH = 400
IMAGE_HEIGHT = 300
IMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT) # (Width, Height)

FEATURE_MAP_WIDTH = 25
FEATURE_MAP_HEIGHT = 18
FEATURE_MAP_SIZE = (25, 18) # (Width, Height)

GRID_CELL_SIZE = ((IMAGE_WIDTH / FEATURE_MAP_WIDTH), (IMAGE_HEIGHT / FEATURE_MAP_HEIGHT))

ANCHOR_WIDTH = 90
ANCHOR_HEIGHT = 120
ANCHOR_SIZE = (ANCHOR_WIDTH, ANCHOR_HEIGHT)

# Channel Constants
CH_OBJ = 0
CH_REG = 1

############################################
# Load the dataset
############################################
ds = HW3Dataset(DATA_PATH)
DATASET_SIZE = len(ds)

SHUFFLE_DATASET = True
BATCH_SIZE = 64
TEST_SPLIT = 0.1
RANDOM_SEED = 777

# Creating data indices for training and validation splits:
indices = list(range(DATASET_SIZE))
split = int(np.floor(TEST_SPLIT * DATASET_SIZE))

if SHUFFLE_DATASET :
    np.random.seed(RANDOM_SEED)
    np.random.shuffle(indices)
    
train_indices, test_indices = indices[split:], indices[:split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
test_sampler = SubsetRandomSampler(test_indices)

train_loader = DataLoader(ds, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=my_collate)
test_loader = DataLoader(ds, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=my_collate)

"""# Helpers"""

def do_for(fn, mat):
  return np.apply_along_axis(fn, 1, mat)

"""## Image Visualization"""

CLASS_COLOR_MAP = {
    1: (255, 0, 0), # Vehicle
    2: (0, 0, 255), # Animals
    3: (0, 255, 0) # People
}

def show_index(dataset, index):
  """Imshow index in dataset with bounding boxes"""
  bbox_viz, mask_viz = _draw_stuff_dataset(dataset, index)
  
  plt.imshow(bbox_viz)
  plt.figure()
  plt.imshow(mask_viz)
  

def _draw_stuff_dataset(dataset, index):
  """Returns a copy of the image at index in dataset with object boundaries drawn"""
  image = dataset.images[index]
  bboxes = dataset.bboxes[index]
  labels = dataset.labels[index]
    
  # We need to use mask indices
  masks = dataset.masks[dataset.mask_indices[index-1]:dataset.mask_indices[index]]
  
  if len(masks.shape) == 2:
    masks = np.expand_dims(masks, 0)
  
  viz = _draw_bounding_boxes(image, bboxes, labels)
  viz_mask = _draw_mask(image, masks, labels)
  
  return viz, viz_mask

def _draw_bounding_boxes(image, bboxes, labels):
    """Draw bounding boxes onto channels-first image. Returns copy of image."""
    ret_copy = image.transpose((1,2,0)).astype(np.uint8).copy()
    for idx in range(labels.shape[0]):
      c1 = tuple(bboxes[idx, :2].astype(int))
      c2 = tuple(bboxes[idx, 2:].astype(int))
      cv2.rectangle(ret_copy, c1, c2, CLASS_COLOR_MAP[labels[idx]], 2)

    return ret_copy
  
  
def _draw_mask(image, masks, labels):
  ret_copy = image.astype(np.uint8).copy()
  
  for idx in range(labels.shape[0]):
    mask = masks[idx]  
    ret_copy[labels[idx]-1] += mask * 128
    
  return ret_copy.transpose((1,2,0))

"""### Data Load Sanity Check"""

for i in range(5,6):
  plt.figure()
  show_index(ds, i)

"""## Flatten Dense Label"""

def flatten_dense_label(dense_label):
  """Flatten a dense label
  
  Input: np.ndarray(5, 18, 25)
  Output: 
        np.ndarray(18 * 25, 5) | Channels: P(obj), x, y, w, h
        np.ndarray(18 * 25, 2) | Row column pairs
  """
  CH, R, C = dense_label.shape

  xv, yv = np.meshgrid(range(C), range(R))
  fat_cells = np.concatenate((np.reshape(yv, (R*C, 1)), np.reshape(xv, (R*C, 1))), axis=1)
  
  fat_cells = fat_cells  
  flat_volume = np.reshape(np.transpose(dense_label, (1,2,0)), (R*C, CH))
    
  return flat_volume, fat_cells

"""## Image Centers and anchor center"""

def center(x1, y1, x2, y2):
  """Returns the center of a pair of coordinates"""
  x_c = (x1 + x2) / 2.
  y_c = (y1 + y2) / 2.
  return x_c, y_c


def _wh_perc(w, h, image_size):
  """Return width/height as percent of image size"""
  w_perc = w / image_size
  h_perc = h / image_size
  return w_perc, h_perc


def _wh(x1, y1, x2, y2):
  """Return width/height"""
  w = x2 - x1
  h = y2 - y1
  return w, h


def wh_labels(labels, image_size=None, get_perc=True):
  """Map labels matrix to width/height matrix"""
  if get_perc:
    wh_fn = lambda row: _wh_perc(*row, image_size)
  else:
    wh_fn = lambda row: _wh(*row)
    
  wh = do_for(wh_fn, labels)
  
  return wh

def area(x1, y1, x2, y2):
  """Return the area of a bounding box"""
  return (x2 - x1) * (y2 - y1)

def box_from_center_wh(x_c, y_c, w, h):
  """Return box coords from center and width/height"""
  x1 = x_c - (w / 2)
  y1 = y_c - (h / 2)  
  x2 = x_c + (w / 2)
  y2 = y_c + (h / 2)
  return x1, y1, x2, y2

def get_iou(box1, box2, eps=1e-5):
  """Get the IoU of two bounding boxes"""
  x1_a, y1_a, x2_a, y2_a = box1
  x1_b, y1_b, x2_b, y2_b = box2
  
  box1_area = area(*box1)
  box2_area = area(*box2)
  
  x1 = max(x1_a, x1_b)
  y1 = max(y1_a, y1_b)
  x2 = min(x2_a, x2_b)
  y2 = min(y2_a, y2_b)
   
  if (x2 - x1 < 0.) or (y2 - y1 < 0.):
    overlap_area = 0.
  else:
    overlap_area = area(x1, y1, x2, y2)
    
  combined_area = box1_area + box2_area - overlap_area
  iou = overlap_area / (combined_area + eps)
  return iou

def ious_from_comp_labels(single_label, comp_labels):
  """Returns the class and IOU value of the ground truth bounding box with max IOU
  
  In: 
    single_label: (4,) - (x1, y1, x2, y2)
    comp_labels: (N, 4) - Matrix of labels. Same as single_label per row
    
  Out: np.array([class_of_max_IOU, max_IOU])
    
  """
  iou_fn = lambda tl: get_iou(single_label, tl)
  ious = do_for(iou_fn, comp_labels)
  
  return ious

"""## Coordinate Transformations"""

def get_anchor_coords(row, col, grid_cell_size, anchor_size):
  
  w_a, h_a = anchor_size
  w_sc, h_sc = grid_cell_size
  
  x1 = int((col * w_sc) + (w_sc / 2) - (w_a / 2))
  x2 = int((col * w_sc) + (w_sc / 2) + (w_a / 2))
  
  y1 = int((row * h_sc) + (h_sc / 2) - (h_a / 2))
  y2 = int((row * h_sc) + (h_sc / 2) + (h_a / 2))
  
  return x1, y1, x2, y2


def get_truth_coords(row, col, anchor, anchor_size, ref_box):
    """Takes an anchor, bounding box, and transforms to truth coordinates
    
    Output:    t_x, t_y, t_w, t_h
    """
    w, h = _wh(*ref_box)
    x_c, y_c = center(*ref_box)
    
    w_a, h_a = anchor_size
    x_c_a, y_c_a = center(*anchor)
    
    t_x = (x_c - x_c_a) / w_a
    t_y = (y_c - y_c_a) / h_a
    
    t_w = np.log(w / w_a)
    t_h = np.log(h / h_a)
  
    return t_x, t_y, t_w, t_h


def get_predicted_coords(row, col, grid_cell_size, anchor_size, pred_coords):
  """Transforms network output in t-space to box coordinates
  
  Input:
        row, col: int
        grid_cell_size: (w, h)
        anchor_size: (w, h)
        pred_coords: (t_x, t_y, t_w, t_h)
        
  Output:  (x1, y1, x2, y2)
  """
  w_a, h_a = anchor_size
  w_sc, h_sc = grid_cell_size
  t_x, t_y, t_w, t_h = pred_coords
  
  anchor = get_anchor_coords(row, col, grid_cell_size, anchor_size)
  
  x_c_a, y_c_a, = center(*anchor)

  x_c = (t_x * w_a) + x_c_a
  y_c = (t_y * h_a) + y_c_a
  
  w = np.exp(t_w) * w_a
  h = np.exp(t_h) * h_a
   
  x1, y1, x2, y2 = box_from_center_wh(x_c, y_c, w, h)
  x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
  
  return x1, y1, x2, y2
  
  
def anchor_in_boundary(x1, y1, x2, y2, width, height):
  if ((x1 < 0) or (y1 < 0) or (x2 > width) or (y2 > height)):
    return 0
  else:
    return 1
  
  
def clip_anchor_to_boundary(x1, y1, x2, y2, width, height):
  x1_r = max(x1, 0)
  y1_r = max(y1, 0)
  x2_r = min(x2, width)
  y2_r = min(y2, height)
  
  return x1_r, y1_r, x2_r, y2_r

"""#### Coordinate space sanity check"""

R, C = 1, 4
GC_SIZE = (16, 16)
ANCHOR_SIZE = (90, 120)
BBOX = [-4, -4, 30, 140]

ANCHOR = get_anchor_coords(R, C, GC_SIZE, ANCHOR_SIZE)
pred_coords = get_truth_coords(R, C, ANCHOR, ANCHOR_SIZE, BBOX)
RET_BOX = get_predicted_coords(R, C, (16, 16), ANCHOR_SIZE, pred_coords)

# THE COORDINATE TRANSFORMATION SHOULD RETURN THE SAME THING!!!
assert (BBOX == list(RET_BOX))

assert (anchor_in_boundary(*BBOX, *_wh(*BBOX)) == 0)

CLIPPED = clip_anchor_to_boundary(*BBOX, *_wh(*BBOX))
assert (list(CLIPPED) == [0, 0, 30, 140])

print("ALL GOOD SIR")

"""## Non-Max Suppression"""

def non_max_suppression(scores, anchors, cluster_thresh=0.5, worst_rows=False, worst_count=3):
  """
  Inputs:
         scores: np.ndarray(N, 1) - Prediction probability
         anchors: np.ndarray(N, 4) - (x1, y1, x2, y2)
         
  Output: 
    Indices of non-suppressed rows
  
  Optional:
    Return n-indices of rows with lowest scores
  """  

  found, found_idx = None, None
  sort_indices = scores.ravel().argsort()[::-1]
  sorted_anchors = anchors[sort_indices]
   
  for i in range(sorted_anchors.shape[0]):
    if found is None:
      # Instantiate the list with the first row
      found_idx = [ sort_indices[i] ]
      found = [ sorted_anchors[i].squeeze() ] 
    else:
      # Check if the current row overlaps with any found bbox
      ious = [get_iou(f, sorted_anchors[i].squeeze()) for f in found]
      # Filter on cluster threshold
      ious = [io for io in ious if io > cluster_thresh]

      if len(ious) == 0:
        found.append(sorted_anchors[i].squeeze())
        found_idx.append(sort_indices[i])

  if worst_rows:
    return found_idx, sort_indices[-worst_count:]
  else:
    return found_idx

"""# RPN Ground Truth

### Single Sample ground truth
"""

def sample_ground_truth(image, bboxes, image_size, grid_cell_size, anchor_size):
      
  CH, R, C = 5, 18, 25  
  dense_label = np.zeros((CH, R, C))
  OBJ_LABEL, NOOBJ_LABEL = 1, -1
  BLOCK = 999
  
  get_anchor_fn = lambda cell: get_anchor_coords(*cell, grid_cell_size, anchor_size)
  cross_boundary_fn = lambda anchor: anchor_in_boundary(*anchor, *image_size)
  
  if bboxes.shape[0] == 0:
    return dense_label
  
  fat_cells = np.array(list(itertools.product(range(R), range(C))))
  anchors = do_for(get_anchor_fn, fat_cells)
    
  # In-boundary anchors
  cb = np.nonzero(do_for(cross_boundary_fn, anchors))[0]
  fat_cells = fat_cells[cb]
  anchors = anchors[cb]
    
  track_max = collections.defaultdict(None) 
  
  for idx in range(anchors.shape[0]):
    row, col = fat_cells[idx]
    
    # Get all of the IoUs w/ bboxes
    ious = ious_from_comp_labels(anchors[idx], bboxes)
    max_iou_id = np.argmax(ious)
    max_iou = ious[max_iou_id]
    
    if max_iou > 0.7:
      
      ANCHOR = anchors[idx]
      BOX = bboxes[max_iou_id]
      
      # Get coordinates in t-space (truth space)
      t_x, t_y, t_w, t_h = get_truth_coords(row, col, ANCHOR, anchor_size, BOX)
      dense_label[:, row, col] = OBJ_LABEL, t_x, t_y, t_w, t_h
            
      # Block this ground truth from assigning labels in auxiliary pass
      track_max[max_iou_id] = (idx, BLOCK) 
    
    else:
      # Set No-object in object channel
      if max_iou < 0.3:
        dense_label[0, row, col] = NOOBJ_LABEL
      
      # Register IoU with each ground truth
      for j, iou in enumerate(ious):
        if not track_max.get(j) or iou > track_max[j][1]:
          track_max[j] = (idx, iou)
        
  # Auxiliary PASS TO FILL IN UNCAUGHT BOUNDARY BOXES
  for idx in track_max.keys():

    if track_max[idx][1] == BLOCK:
      continue

    row, col = fat_cells[track_max[idx][0]]
    
    ANCHOR = anchors[track_max[idx][0]]
    BOX = bboxes[idx]
    
    # Get coordinates in t-space (truth space    
    t_x, t_y, t_w, t_h = get_truth_coords(row, col, ANCHOR, anchor_size, BOX)
    dense_label[:, row, col] = OBJ_LABEL, t_x, t_y, t_w, t_h
    
  return dense_label

"""## Do/Save RPN Ground Truth"""

# rpn_ground_truth = np.empty_like(ds.labels)

# for i in range(len(ds)):
    
#     rpn_ground_truth[i] = sample_ground_truth(
#         ds.images[i],
#         ds.bboxes[i],
#         IMAGE_SIZE,
#         GRID_CELL_SIZE,
#         ANCHOR_SIZE)
  
# np.save(os.path.join(DATA_PATH, "rpn_ground_truth_2.npy"), rpn_ground_truth)

"""## Ground Truth Sanity Check"""

cross_boundary_fn = lambda anchor: anchor_in_boundary(*anchor, *IMAGE_SIZE)

IDX = 111

# Original bounding boxes
dense_label = sample_ground_truth(
    ds[IDX]['images'],
    ds[IDX]['bboxes'],
    IMAGE_SIZE,
    GRID_CELL_SIZE,
    ANCHOR_SIZE)

t_coords, fat_cells = flatten_dense_label(dense_label)

scores, t_coords = np.split(t_coords, indices_or_sections=[1], axis=1)

obj_ind = (scores.squeeze() == 1).nonzero()

print(len(obj_ind[0]))

t_coords, fat_cells = t_coords[obj_ind], fat_cells[obj_ind]

anchors = np.empty_like(t_coords)

# Get the anchors 
for jdx in range(fat_cells.shape[0]):
  row, col = fat_cells[jdx]
  anchors[jdx] = get_anchor_coords(row, col, GRID_CELL_SIZE, ANCHOR_SIZE)

# Final list of predicted anchors filtering out cross boundary
in_bounds = np.nonzero(do_for(cross_boundary_fn, anchors))[0]
fat_cells = fat_cells[in_bounds]
anchors = anchors[in_bounds]  
 
all_boxes = np.concatenate([anchors,  ds[IDX]['bboxes']])
labels = np.concatenate([np.ones(len(anchors)), np.ones(len(ds[IDX]['bboxes']))*2])

plt.imshow(_draw_bounding_boxes(ds[IDX]['images'], all_boxes, labels))
# plt.savefig("rpn_visualization_{}.png".format(IDX), dpi=600)
# files.download("rpn_visualization_{}.png".format(IDX))

"""# FasterRCNN"""

from torch.nn import Conv2d, BatchNorm2d, MaxPool2d
import torch.nn.functional as F
import torch.nn as nn

class FasterRCNN(torch.nn.Module):
  def __init__(self):
    super(FasterRCNN, self).__init__()

    self.conv1 = Conv2d(3, 16, 5, stride=1, padding=5//2)
    self.conv1_bn = BatchNorm2d(16)
    
    self.conv2 = Conv2d(16, 32, 5, stride=1, padding=5//2)
    self.conv2_bn = BatchNorm2d(32)
    
    
    self.conv3 = Conv2d(32, 64, 5, stride=1, padding=5//2)
    self.conv3_bn = BatchNorm2d(64)
    
    self.conv4 = Conv2d(64, 128, 5, stride=1, padding=5//2)
    self.conv4_bn = BatchNorm2d(128)    

    self.conv5 = Conv2d(128, 256, 5, stride=1, padding=5//2)
    self.conv5_bn = BatchNorm2d(256)
    
    self.pool = MaxPool2d(2, stride=2, padding=0)
    
    # Intermediate layer
    self.conv6 = Conv2d(256, 256, 3, stride=1, padding=3//2)
    self.conv6_bn = BatchNorm2d(256)    
    
    # Proposal Classifier
    self.conv7 = Conv2d(256, 1, 1, stride=1, padding=0)
    
    # Regression Classifier
    self.conv8 = Conv2d(256, 4, 1, stride=1, padding=0)
    
  def forward(self, x):
    x = self.pool(F.relu(self.conv1_bn(self.conv1(x))))
    x = self.pool(F.relu(self.conv2_bn(self.conv2(x))))
    x = self.pool(F.relu(self.conv3_bn(self.conv3(x))))
    x = self.pool(F.relu(self.conv4_bn(self.conv4(x))))
    x = F.relu(self.conv5_bn(self.conv5(x)))
    x = F.relu(self.conv6_bn(self.conv6(x)))

    # Proposal
    prop = torch.sigmoid(self.conv7(x))
    
    # Regressor
    reg = self.conv8(x)
    
    return reg, prop

USE_PRETRAINED = True

net = FasterRCNN()
net.to(DEVICE)
print("Instantiated FasterRCNN")

if USE_PRETRAINED:
    try:
      net.load_state_dict(torch.load(os.path.join(CHECKPOINT_PATH, '2019-10-29_19:30:02.ckpt')))
      print("Loaded weights from checkpoint: {}".format('2019-10-29_19:30:02.ckpt'))
    except:
      print("Checkpoint not found: {}".format('faster_rcnn_pretrained.ckpt'))

# Optional debugging summary
# summary(net, (3, 300, 400))

"""# Loss"""

class RPN_Regression_Loss(nn.Module):
  
  def __init__(self):
    super(RPN_Regression_Loss, self).__init__()
    self.criterion_prop = nn.BCELoss(reduction='sum')
    self.criterion_reg = nn.SmoothL1Loss(reduction='sum')
    self.anchor_total = 256
    
  def forward(self, predicted, truth):
    
    reg, prop = predicted
    
    # Split truth into proposal and regression channels
    truth_prop = truth[:, CH_OBJ] # channel 0
    truth_reg = truth[:, CH_REG:] # channel 1:4
    
    obj_mask_1d = (truth_prop == 1)        # mask on labels {+1}
    noobj_mask_1d = (truth_prop == -1)     # mask on labels {-1}
    not_noobj_mask_1d = (truth_prop != -1) # mask on labels {0, +1}

    image_centric_mask = torch.zeros_like(noobj_mask_1d)   
    
    obj_mask_4d = torch.unsqueeze((truth_prop == 1) , 1).repeat(1,4,1,1)
    noobj_mask_4d = torch.unsqueeze((truth_prop == -1), 1).repeat(1,4,1,1)

    obj_ct = obj_mask_1d.sum().item()      # num. hard positives
    noobj_ct = noobj_mask_1d.sum().item()  # num. hard negatives

    # BCE loss masked on object samples {+1}
    loss_prop_POS = self.criterion_prop(
        torch.mul(prop.squeeze(), obj_mask_1d),
        torch.mul(truth_prop, obj_mask_1d))
 
    SAMPLE_CT = min((self.anchor_total - obj_ct), noobj_ct)
    SUB_IDX = np.random.choice(noobj_ct, SAMPLE_CT, replace=False)
    image_centric_mask[noobj_mask_1d.nonzero()[SUB_IDX].split(1, dim=1)] = 1
    
    # BCE loss masked on no-object samples {-1}|
    loss_prop_NEG = self.criterion_prop(
        torch.mul(prop.squeeze(), image_centric_mask),
        torch.mul(torch.mul(truth_prop, not_noobj_mask_1d), image_centric_mask))

#     loss_prop_NEG = self.criterion_prop(
#         torch.mul(prop.squeeze(), noobj_mask_1d),
#         torch.mul(torch.mul(truth_prop, not_noobj_mask_1d), noobj_mask_1d)) / noobj_ct       
    
    # Imbalance corrected proposal loss - 
    # TODO: I DON'T THINK THIS CONSTANT IS CORRECT - the norm constants on POS and NEG are involved
    loss_prop_TOT = (loss_prop_POS + loss_prop_NEG) / (obj_ct + SAMPLE_CT)

    # Regression loss. Divide by object count (# {+1}) as norm constant.
    loss_reg = self.criterion_reg(
        torch.mul(reg, obj_mask_4d),
        torch.mul(truth_reg, obj_mask_4d)) / obj_ct

    # loss_total = (loss_prop_TOT + (((obj_ct + SAMPLE_CT) / obj_ct) * loss_reg)) / (obj_ct + SAMPLE_CT)
    loss_total = (((obj_ct + SAMPLE_CT) / obj_ct) * loss_prop_TOT + loss_reg)
    # loss_total = loss_reg + loss_prop_TOT

    loss_dict = {
        # "proposal_positive": loss_prop_POS,
        # "proposal_negative": loss_prop_NEG,
        "proposal_total": loss_prop_TOT,
        "regression": loss_reg,
        "total": loss_total,
        "object_count": obj_ct,
        "noobject_count": SAMPLE_CT
    }
    
    return loss_dict

"""## Inference (test loss)"""

def inference(net, loader):
  """Returns mean loss per sample in loader"""
  
  running_loss, running_reg, running_prop = 0.0, 0.0, 0.0
    
  with torch.no_grad():
    
    for idx, (batch, pad_lengths) in enumerate(loader):

      images, truth, _, _, _ = batch    
      
      images = images.to(device=DEVICE, dtype=torch.float)
      truth = truth.to(device=DEVICE, dtype=torch.float)
      
      reg, prop = net(images)
      
      RPN_LOSS = RPN_Regression_Loss()  

      loss_dict = RPN_LOSS((reg, prop), truth)

      running_loss += loss_dict["total"].item()
      running_prop += loss_dict["proposal_total"].item()
      running_reg += loss_dict["regression"].item()

  tl = running_loss / (len(loader))
  pl = running_prop / (len(loader))
  rl = running_reg / (len(loader))
  
  return tl, pl, rl

def proposal_confusion_matrix(net, loader):
  """Returns mean loss per sample in loader"""
        
  TP, FP, TN, FN = 0, 0, 0, 0

  with torch.no_grad():

    for idx, (batch, pad_lengths) in enumerate(loader):

      images, truth, _, _, _ = batch    
      
      images = images.to(device=DEVICE, dtype=torch.float)
      truth = truth.to(device=DEVICE, dtype=torch.float)
      
      truth_prop = truth[:, CH_OBJ]

      obj_mask_1d = (truth_prop == 1)    # mask on labels {+1}
      noobj_mask_1d = (truth_prop == -1) # mask on labels {-1}

      truth_prop[truth_prop == -1] = 0

      _, prop = net(images)

      # Threshold prediction at 0.5
      prop = (prop > 0.5).squeeze()
      
      tp = torch.mul(truth_prop.eq(prop), obj_mask_1d).sum().item()
      fp = obj_mask_1d.sum().item() - tp

      tn = torch.mul(truth_prop.eq(prop), noobj_mask_1d).sum().item()
      fn = noobj_mask_1d.sum().item() - tn

      TP += tp
      FP += fp
      TN += tn
      FN += fn

  accuracy = (TP + TN) / (TP + FP + TN + FN)
  precision = TP / (TP + FP)
  recall = TP / (TP + FN)
  
  return accuracy, precision, recall

def regression_confusion_matrix(net, loader):
  """Returns mean loss per sample in loader"""
  clip_anchor_fn = lambda anchor: clip_anchor_to_boundary(*anchor, IMAGE_WIDTH, IMAGE_HEIGHT)
      
  TP, FP, TN, FN = 0, 0, 0, 0

  with torch.no_grad():

    for idx, (batch, pad_lengths) in enumerate(loader):

      images, truth, _, _, _ = batch    
      
      images = images.to(device=DEVICE, dtype=torch.float)
      truth = truth.to(device=DEVICE, dtype=torch.float)
      
      truth_prop = truth[:, CH_OBJ]

      obj_mask_1d = (truth_prop == 1)    # mask on labels {+1}
      noobj_mask_1d = (truth_prop == -1) # mask on labels {-1}

      truth_prop[truth_prop == -1] = 0

      pred_iou_post_nms = torch.zeros_like(truth_prop)

      reg, prop = net(images)

      # Threshold prediction at 0.5
      prop = (prop > 0.5).float()

      for SDX in range(images.shape[0]):

        dense_label = torch.squeeze(torch.cat((prop, reg), 1)[SDX]).detach().cpu().numpy()

        t_coords, fat_cells = flatten_dense_label(dense_label)
          
        scores, t_coords = np.split(t_coords, indices_or_sections=[1], axis=1)
        
        t_coord_to_anchor_fn = lambda idx: get_predicted_coords(
            fat_cells[idx, 0],
            fat_cells[idx, 1],
            GRID_CELL_SIZE,
            ANCHOR_SIZE,
            t_coords[idx].squeeze())
        
        row_indices = np.expand_dims(np.arange(0, t_coords.shape[0]), -1)
        
        # Convert t_coordinates to anchors
        anchors = do_for(t_coord_to_anchor_fn, row_indices)
        # Clip anchors to boundary
        anchors = do_for(clip_anchor_fn, anchors)
        
        # NMS on the list of anchors. These are predicted positives in regression
        kept_anchors_idx = non_max_suppression(scores, anchors, cluster_thresh=0.7)
        
        for ki in kept_anchors_idx:
          pred_iou_post_nms[SDX, fat_cells[ki, 0], fat_cells[ki, 1]] = 1

      prop = prop.squeeze()

      # For regression, incorporate accuracy of NMS filtered predictions
      nms_filtered_prop = torch.mul(prop, pred_iou_post_nms)

      tp = torch.mul(truth_prop.eq(nms_filtered_prop), obj_mask_1d).sum().item()
      fp = obj_mask_1d.sum().item() - tp

      tn = torch.mul(truth_prop.eq(nms_filtered_prop), noobj_mask_1d).sum().item()
      fn = noobj_mask_1d.sum().item() - tn

      TP += tp
      FP += fp
      TN += tn
      FN += fn

  accuracy = (TP + TN) / (TP + FP + TN + FN)
  precision = TP / (TP + FP)
  recall = TP / (TP + FN)
  
  return accuracy, precision, recall

"""# Training

## Proposal Classifier Training
"""

EPOCHS = 10
LOG_LOSS_MINI_BATCHES = 25

# Accumulated Proposal classifier losses
train_proposal_total = []

# Loss Function
RPN_LOSS = RPN_Regression_Loss()  

# Optimizer with scheduler
optimizer = torch.optim.SGD(net.parameters(), lr=2e-3)
torch.optim.lr_scheduler.StepLR(optimizer, 3, gamma=0.1)

for epoch in range(EPOCHS):

    running_prop, prop_tot_loss = 0.0, 0.0

    for idx, (batch, pad_lengths) in enumerate(train_loader):

        images, truth, _, _, _ = batch
      
        # zero the parameter gradients
        optimizer.zero_grad()

        images = images.to(device=DEVICE, dtype=torch.float)
        truth = truth.to(device=DEVICE, dtype=torch.float)
        
        # Forward
        reg, prop = net(images)
        
        # Loss
        loss_dict = RPN_LOSS((reg, prop), truth)

        # Backpropogate on proposal classifier loss
        loss_dict["proposal_total"].backward()
        optimizer.step()

        # Track losses
        prop_tot_loss += loss_dict["proposal_total"].item()
        running_prop += loss_dict["proposal_total"].item()
        
        if idx % LOG_LOSS_MINI_BATCHES == (LOG_LOSS_MINI_BATCHES - 1):    
          print("Epoch: {}, Batch: {} | Prop: {:.3f}".format(
            epoch + 1, idx + 1,
            running_prop / LOG_LOSS_MINI_BATCHES,       
          ))
          running_prop = 0.0
    
    train_proposal_total.append(prop_tot_loss / len(train_loader))
    prop_tot_loss = 0.0

fig = plt.figure()
ax = plt.subplot(111)

plt.title("RPN Proposal Classifier Training Loss")
plt.xlabel("Epoch")
plt.ylabel("Mean loss / sample")
ax.plot(train_proposal_total, label="Proposal train loss", marker='o')
ax.legend( prop={'size': 12}, framealpha=1.0)
ax.grid(which='major')

import matplotlib.ticker as ticker
tick_spacing = 1
ax.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))

# plt.savefig("unscaled_proposal_loss.png", dpi=600)
# files.download("unscaled_proposal_loss.png")

"""## Regression Training"""

EPOCHS = 10
LOG_LOSS_MINI_BATCHES = 25

# Accumulated Proposal classifier losses
train_regression_total = []

# Loss Function
RPN_LOSS = RPN_Regression_Loss()  

# Optimizer with scheduler
optimizer = torch.optim.SGD(net.parameters(), lr=2e-3)
torch.optim.lr_scheduler.StepLR(optimizer, 3, gamma=0.1)

for epoch in range(EPOCHS):

    running_reg, reg_tot_loss = 0.0, 0.0

    for idx, (batch, pad_lengths) in enumerate(train_loader):

        images, truth, _, _, _ = batch
      
        # zero the parameter gradients
        optimizer.zero_grad()

        images = images.to(device=DEVICE, dtype=torch.float)
        truth = truth.to(device=DEVICE, dtype=torch.float)
        
        # Forward
        reg, prop = net(images)
        
        # Loss
        loss_dict = RPN_LOSS((reg, prop), truth)

        # Backpropogate on regression loss
        loss_dict["regression"].backward()
        optimizer.step()

        # Track losses
        reg_tot_loss += loss_dict["regression"].item()
        running_reg += loss_dict["regression"].item()
        
        if idx % LOG_LOSS_MINI_BATCHES == (LOG_LOSS_MINI_BATCHES - 1):    
          print("Epoch: {}, Batch: {} | Reg: {:.3f}".format(
            epoch + 1, idx + 1,
            running_reg / LOG_LOSS_MINI_BATCHES,       
          ))
          running_reg = 0.0
    
    train_regression_total.append(reg_tot_loss / len(train_loader))
    reg_tot_loss = 0.0

fig = plt.figure()
ax = plt.subplot(111)

plt.title("RPN Unscaled Loss Components")
plt.xlabel("Epoch")
plt.ylabel("Mean loss / sample")
ax.plot(train_regression_total, label="Regression train loss", marker='o', color='orange')
ax.plot(train_proposal_total, label="Proposal train loss", marker='o')
ax.legend( prop={'size': 12}, framealpha=1.0)
ax.grid(which='major')

import matplotlib.ticker as ticker
tick_spacing = 1
ax.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))

# plt.savefig("both_unscaled_loss_components.png", dpi=600)
# files.download("both_unscaled_loss_components.png")

"""## Joint Proposal & Regression Training"""

SAVE_MODEL = False
EPOCHS = 10
LOG_LOSS_MINI_BATCHES = 25

if not USE_PRETRAINED:

  # Accumulated losses
  train_total_losses = []

  test_total_losses = []
  test_proposal_total = []
  test_regression = []

  # Loss Function
  RPN_LOSS = RPN_Regression_Loss()  
  
  # Optimizer with scheduler
  optimizer = torch.optim.SGD(net.parameters(), lr=3e-4)
  torch.optim.lr_scheduler.StepLR(optimizer, 3, gamma=0.1)

  for epoch in range(EPOCHS):

      running_loss, total_loss = 0.0, 0.0

      for idx, (batch, pad_lengths) in enumerate(train_loader):

          images, truth, _, _, _ = batch
        
          # zero the parameter gradients
          optimizer.zero_grad()

          images = images.to(device=DEVICE, dtype=torch.float)
          truth = truth.to(device=DEVICE, dtype=torch.float)
          
          # Forward
          reg, prop = net(images)
          
          # Loss
          loss_dict = RPN_LOSS((reg, prop), truth)

          # Backward + step
          loss_dict["total"].backward()
          optimizer.step()

          # Track losses
          total_loss += loss_dict["total"].item()
          running_loss += loss_dict["total"].item()
          
          if idx % LOG_LOSS_MINI_BATCHES == (LOG_LOSS_MINI_BATCHES - 1):    

            print("Epoch: {}, Batch: {} | Tot: {:.3f}".format(
              epoch + 1, idx + 1,
              running_loss / LOG_LOSS_MINI_BATCHES,          
            ))

            running_loss = 0.0
      
      tl, pl, rl = inference(net, test_loader)

      test_total_losses.append(tl)
      test_proposal_total.append(pl)
      test_regression.append(rl)
      
      train_total_losses.append(total_loss / (len(train_loader)))
      
      total_loss = 0.0
    
  if SAVE_MODEL:   
    checkpoint_file = datetime.now().strftime("%Y-%m-%d_%H:%M:%S.ckpt")
    torch.save(net.state_dict(), os.path.join(CHECKPOINT_PATH, checkpoint_file))

fig = plt.figure()
ax = plt.subplot(111)

plt.title("Faster-RCNN Train/Test Losses")
plt.xlabel("Epoch")
plt.ylabel("Mean loss / sample")

ax.plot(train_total_losses, label="Train loss", color='b')
ax.plot(test_total_losses, label="Test loss", color='r')

# ax.plot([2*t for t in test_proposal_total], label="Test proposal loss", linestyle='-.')
# ax.plot(test_regression, label="Test regression loss", linestyle='-.')


ax.legend( prop={'size': 10}, framealpha=1.0)
ax.grid(which='major')

import matplotlib.ticker as ticker
tick_spacing = 1
ax.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))  

# plt.savefig("total_loss.png", dpi=600)
# files.download("total_loss.png")

"""## Confusion-Matrix Classifier Metrics"""

## UNCOMMENT IF YOU ACTUALLY WANT TO RUN THIS!

accuracy, precision, recall = proposal_confusion_matrix(net, test_loader)
print(accuracy, precision, recall)

"""## Confusion-Matrix Regressor Metrics"""

## UNCOMMENT IF YOU ACTUALLY WANT TO RUN THIS!

# accuracy, precision, recall = regression_confusion_matrix(net, test_loader)
# print(accuracy, precision, recall)

"""# Examining Network Output"""

KEEP_N = 3

print("Demonstrating inference from some of {} test samples".format(len(test_indices)))

clip_anchor_fn = lambda anchor: clip_anchor_to_boundary(*anchor, IMAGE_WIDTH, IMAGE_HEIGHT)

for idx, (batch, pad_lengths) in enumerate(test_loader):

  if idx <=3:
    continue
  elif idx >= 8:
    break

  img, _, _, bboxes, labels = batch    
      
  images = img.to(device=DEVICE, dtype=torch.float)

  net.eval()
  reg, prop = net(images)

  dense_label = torch.squeeze(torch.cat((prop, reg), 1)[0]).detach().cpu().numpy()

  t_coords, fat_cells = flatten_dense_label(dense_label)
    
  scores, t_coords = np.split(t_coords, indices_or_sections=[1], axis=1)
  
  t_coord_to_anchor_fn = lambda idx: get_predicted_coords(
      fat_cells[idx, 0],
      fat_cells[idx, 1],
      GRID_CELL_SIZE,
      ANCHOR_SIZE,
      t_coords[idx].squeeze())
  
  row_indices = np.expand_dims(np.arange(0, t_coords.shape[0]), -1)
  
  # Convert t_coordinates to anchors
  anchors = do_for(t_coord_to_anchor_fn, row_indices)
  # Clip anchors to boundary
  anchors = do_for(clip_anchor_fn, anchors)
  
  # NMS on the list of anchors
  kept_anchors_idx, worst_anchors_idx = non_max_suppression(scores, anchors, cluster_thresh=0.7, worst_rows=True)
  
  worst_anchors = anchors[worst_anchors_idx]
  anchors = anchors[kept_anchors_idx]
  
  n_anchors = anchors[:KEEP_N]
  
  all_boxes = np.concatenate([n_anchors, worst_anchors, bboxes[0].squeeze(0).numpy()], axis=0)


  all_labels = np.concatenate([
                           np.ones(len(n_anchors)),
                           np.ones(len(worst_anchors)) * 2,
                           np.ones(len(bboxes[0])) * 3])
  
  print(img[0].squeeze().shape)

  plt.figure()  
  plt.imshow(_draw_bounding_boxes(img[0].squeeze().numpy(), all_boxes, all_labels))
  # plt.savefig("rpn_outputs_{}.png".format(idx), dpi=600)
  # files.download("rpn_outputs_{}.png".format(idx))

"""# Dataset Histograms"""

widths, heights, aspects = [], [], []

for i in range(len(ds)):
  wh = wh_labels(ds[i]['bboxes'], get_perc=False)
  widths.append(wh[:, 0])
  heights.append(wh[:, 1])
  aspects.append(wh[:, 0] / wh[:, 1])
  
widths = np.concatenate(widths).ravel()
heights = np.concatenate(heights).ravel()
aspects = np.concatenate(aspects).ravel()

bin_ct = [IMAGE_WIDTH / 20, IMAGE_HEIGHT / 20]

h = plt.hist2d(widths, heights, bins=bin_ct)
plt.title("Dataset Histogram")
plt.xlabel("Width")
plt.ylabel("Height")

# plt.savefig("histogram.png", dpi=600)
# files.download("histogram.png") 

h = h[0]

max_w_ind, max_h_ind = np.unravel_index(h.argmax(), h.shape)

mode_w = ((max_w_ind + 1) * 20) + 10
mode_h = ((max_h_ind + 1) * 20) + 10

for index_id, (batch, pad_lengths) in enumerate(train_loader):
  
  images, truth, masks, bboxes, labels = batch
  
  for i in range(len(batch)):
    print(batch[i].shape, batch[i].dtype)
    
  for k,v in pad_lengths.items():
    print(k)
    print(v)
  
  break

#           loss_prop = criterion_prop(
#               torch.mul(prop.squeeze(), loss_mask),
#               torch.mul(torch.mul(truth[:, CH_OBJ], not_noobj_mask), loss_mask))
         
  
#             tl = test_loss(net, validation_loader, criterion)
#             batch_losses.append(running_loss / LOG_LOSS_MINI_BATCHES)
#             test_losses.append(tl)  


#     loss_prop_NEG = self.criterion_prop(
#         torch.mul(prop.squeeze(), noobj_mask_1d),
#         torch.mul(torch.mul(truth_prop, not_noobj_mask_1d), noobj_mask_1d))       
    
             
#     a = torch.threshold(torch.randn(5, 5, 5), 1, 0)
#     idx = torch.nonzero(a)[:5]
#     print(idx)
#     idx = idx.split(1, dim=1)
#     print(idx)
#     a[idx] = 12
#     print(a)

    
    #         loss_prop_TOT = ((noobj_ct / obj_ct) * loss_prop_POS + loss_prop_NEG) / (noobj_ct + obj_ct)

